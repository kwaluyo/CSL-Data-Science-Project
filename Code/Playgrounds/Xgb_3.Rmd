---
title: "Xgb_2"
output: pdf_document
---

```{r}
#install.packages('xgboost') 
library(xgboost)
library(caret)
```


```{r}
load("exp_blood_t.Rdata")
load("exp_other_t.Rdata")
#str(exp_blood_t)
#str(exp_other_t)
dim(exp_blood_t)
dim(exp_other_t)
```

```{r}
ALL_GENE = ncol(exp_other_t)
N = nrow(exp_other_t)
```


```{r}
NUM_GENE = ALL_GENE
exp_combined = cbind(exp_blood_t[,1:ALL_GENE], exp_other_t[,1:NUM_GENE])
```

```{r}
# Matrix

set.seed(321)

rows <- sample(nrow(exp_combined))
exp_combined_shuffled <- exp_combined[rows, ]


data = as.matrix(exp_combined_shuffled)

dimnames(data) = NULL
```

```{r}
# Scaling
data = scale(data)
```

```{r}
dim(exp_blood_t)
dim(data)
```

```{r}
# Partition

train_proportion = 0.8
test_proportion = 1-train_proportion

n_train = round(N*train_proportion)
n_test = N - n_train

train_x = data[1:n_train, 1:ALL_GENE]
test_x = data[(n_train+1):N, 1:ALL_GENE]

train_y = data[1:n_train, (ALL_GENE+1):(ALL_GENE+NUM_GENE)]
test_y = data[(n_train+1):N, (ALL_GENE+1):(ALL_GENE+NUM_GENE)]

```

```{r}
dim(train_x)
dim(test_x)

dim(train_y)
dim(test_y)
```


```{r}
a = matrix(nrow=3, ncol=2)
b = c(1, 2, 3)
c = c(4, 5, 6)
a[,1] = b
a[,2] = c
```

```{r}
#Kartika's version

NCOL = 1
# change NCOL with ncol(train_y)

models<-list()
mae_list = c()
pred_list = matrix(nrow = n_test, ncol=NCOL)

#setting up the cross-validation hyperparamete search 

xgb_grid_1 = expand.grid(
  nrounds = c(100, 500, 1000),
  eta = c(0.001, 0.01, 0.1),
  max_depth = c(2, 4, 6, 8, 10),
  gamma = c(0, 1, 2, 5)
)

#training control parameters

xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "all",
  classProbs = FALSE,
  summaryFunction = twoClassSummary,
  allowParallel = TRUE,
)

for (i in 1:NCOL){
  
  xgb_train = xgb.DMatrix(data = train_x, label = train_y[,i])
  xgb_test = xgb.DMatrix(data = test_x, label = test_y[,i])
  
  models[[i]] <- train(xgb_train, xgb_test,
                       trControl = xgb_trcontrol_1,
                       tuneGrid = xgb_grid_1,
                       method = "xgbTree")
  
 # pred_y <- predict(models[[i]], xgb_test)
  
# mae = caret::MAE(test_y[,i], pred_y)
#  mae_list = c(mae_list, mae)
  
 # pred_list[,i] = pred_y

}
```

```{r}
pred_list
```


```{r}
mae_list
(avg_mae = mean(mae_list))
```

```{r}
cor(c(pred_list), c(test_y[,1:NCOL]))
```

NOTE:
do cross validation for tuning the hyperparameters
how? 
split the training set into k-fold
iterate through the folds for each hyperparameter values you want to tune
dont forget to save all models
take the average of the validation mae's
pick the best_model with the lowest mae
do: pred_y <- predict(best_model, xgb_test)

hope this helps!




