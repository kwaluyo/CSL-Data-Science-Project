---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


https://www.rdocumentation.org/packages/kerastuneR/versions/0.1.0.3


```{r}
#install.packages("keras")
#install.packages("mlbench")
#install.packages("dplyr")
#install.packages("magrittr")
#install.packages("neuralnet")
#require(devtools)
#install_version("echarts4r", version = "0.3.3", repos = "http://cran.us.r-project.org")
#devtools::install_github('henry090/kerastuneR')
#kerastuneR::install_kerastuner()
```

```{r}
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
library(tensorflow)
library(kerastuneR)
```

```{r}
load("./exp_blood.Rdata")
load("./exp_other.Rdata")
```

```{r}
exp_blood_t = t(exp_blood)
exp_other_t = t(exp_other)
```

```{r}
ALL_GENE = ncol(exp_other_t)
N = nrow(exp_other_t)
```


```{r}
NUM_GENE = ALL_GENE
exp_combined = cbind(exp_blood_t[,1:ALL_GENE], exp_other_t[,1:NUM_GENE])
```



```{r}
# Matrix

set.seed(321)

rows <- sample(nrow(exp_combined))
exp_combined_shuffled <- exp_combined[rows, ]


data = as.matrix(exp_combined_shuffled)

dimnames(data) = NULL
```

```{r}
# Scaling
data = scale(data)
```

```{r}
dim(exp_blood_t)
dim(data)
```

```{r}
# Partition
train_proportion = 0.8
test_proportion = 1-train_proportion

n_train = round(N*train_proportion)
n_test = N - n_train

training = data[1:n_train, 1:ALL_GENE]
test = data[(n_train+1):N, 1:ALL_GENE]

trainingtarget = data[1:n_train, (ALL_GENE+1):(ALL_GENE+NUM_GENE)]
testtarget = data[(n_train+1):N, (ALL_GENE+1):(ALL_GENE+NUM_GENE)]

```


```{r}
dim(training)
dim(test)

dim(trainingtarget)
dim(testtarget)
```
```{r}
numLayers = seq(2,20,2)
units = seq(20,500,20)
```





```{r}
a = c(1, 2, 3, 4)
a[2]
```



```{r}
hp = HyperParameters()
hp$Choice('learning_rate', c(1e-3, 1e-5))
hp$Choice('num_layers', numLayers)
hp$Choice('units', units)
```




```{r}
my_model = function(hp) {
  
  model = keras_model_sequential() 
  model %>% layer_dense(1000, input_shape = c(ALL_GENE), activation='relu')
  
  for (i in 1:(hp$get('num_layers')) ) {
    model %>% layer_dense(hp$get('units'), activation='relu') 
  } 
  
  model %>% layer_dense(units = NUM_GENE, activation='relu')   %>%
    
    compile(
      optimizer = tf$keras$optimizers$Adam(hp$get('learning_rate')),
      # hp$get('learning_rate')
      loss = 'mae',
      metrics = 'mse') 
  return(model)
  
}
```



```{r}
tuner = RandomSearch(
  hypermodel =  my_model,
  max_trials = 1000,
  hyperparameters = hp,
  tune_new_entries = T,
  objective = 'val_mse')
```


```{r}
a = rep(NA, dim(test)[1])
test_missing = test
test_missing[,1] = a
```




```{r}
tuner %>% fit_tuner(x = training,
                    y = trainingtarget, 
                    validation_data = list(test, testtarget),
                    epochs = 3)
```

```{r}
tuner %>% search_summary()
```

```{r}
result = kerastuneR::plot_tuner(tuner)
# the list will show the plot and the data.frame of tuning results
result 
```
```{r}
write.csv(result[[2]], "Task5_NN3_Result.csv")
```


```{r}
best_5_models = tuner %>% get_best_models(5)
best_5_models[[1]] %>% plot_keras_model()
```




`


















