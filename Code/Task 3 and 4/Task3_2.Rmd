---
title: "Task 03 & Task 04 - Data Science Project"
author: "Kartika Waluyo & Vrinda Rajendar Rajanahally"
date: "1000555 & 1129446"
output:
  pdf_document: default
  word_document:
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Task Description 

Task 03 aims at selecting appropriate sample size for training, validation and test sets defined for the chosen tissues i.e. Lung, Skin - Not sun exposed, and Nerve - Tibial. 

Task 04 deals with the selection of modelling methods that can be used to predict normally distributed output from normally distributed input.

## Task 03: Defining the sizes of training, validation and test sets 

The first half of this report will cover the approach and strategy behind choosing the appropriate proportions for the different sets. The type of sets involved in this project are: 

1. Training set: the samples belonging to this set will be used to create the model. It is typically the set with the largest sample size. 
2. Validation set: this set will be used to tune the parameters of the model and to further optimise it. Furthermore, this set will also be used to choose the best model between the 2 models (Random Forest and Neural Network) which are going to be further discussed in Task 04.
3. Test set: the model is finally run on this set, which it has never seen. It is used to assess the performance of the model and the accuracy of results. 

We could not find a strong agreement after referring various sources and published papers on selecting appropriate proportion to define Training, Validation and Test sets. Hence, our approach to choosing the correct proportions for the sets was to ensure that each one of them has an ample amount of donors. Choosing an arbitrary number of 30 samples, our main aim is to ensure that the test set (the smallest across the three) must have a sample size of at least 30 donors overlapping with Whole Blood. 

```{r echo=FALSE}
#indexes
LUNG_INDEX = 37
NERVE_INDEX = 40
SKIN_INDEX = 45
```

```{r echo=FALSE}
library(ggplot2)
library(tidyr)
```

```{r echo=FALSE}
files <- list.files("~/CSL/TMM_norm_filt_counts")
#files <- list.files("~/TMM_norm_filt_counts")
```

```{r echo=FALSE}
require("edgeR",quietly=TRUE)

tissue_list <- list()
file_name <- list()

c = 1
for (i in files) {
  load(paste("./TMM_norm_filt_counts/",i, sep=""))
  tissue_list[[i]] <- df
  file_name[c] = i
  c = c + 1
}
```

```{r echo=FALSE}
tissue_names = list()
global_tissue_names = list()

i = 1
for (a in file_name) {
  a = sub("TMM_filt_counts_","",a)
  a = sub(".Rdata","",a)
  tissue_names[[i]] = a
  a = unlist(strsplit(a, split=' -', fixed=TRUE))[1]
  global_tissue_names[[i]] = a
  
  i = i + 1
}
tissue_names = as.character(tissue_names)
global_tissue_names = as.character(global_tissue_names)

```

```{r echo=FALSE}
load(paste("./TMM_norm_filt_counts/TMM_filt_counts_Whole Blood.Rdata"))
df_blood = df
rownames(df_blood$cpm_tmm) <- df_blood$genes$gene_id 
colnames(df_blood$cpm_tmm) <- df_blood$samples$SUBJID
```

```{r echo=FALSE}
expression_list <- list()
# expression_list contains expressions from all tissues, the column is the shared subjects with WB

j = 1
for (df_other in tissue_list) {
 
  rownames(df_other$cpm_tmm) <- df_other$genes$gene_id 
  colnames(df_other$cpm_tmm) <- df_other$samples$SUBJID 
    
  shared_genes <- df_blood$genes$gene_id[df_blood$genes$gene_id%in%df_other$genes$gene_id] 
  shared_subjects <- df_blood$samples$SUBJID[df_blood$samples$SUBJID%in%df_other$samples$SUBJID]
  
    
  exp_blood <- df_blood$cpm_tmm[shared_genes,shared_subjects] 
  exp_other <- df_other$cpm_tmm[shared_genes,shared_subjects] 
  
  expression_list[[j]] = as.data.frame(exp_other)
  
  
  j = j + 1
}

```

```{r echo=FALSE}
prop_7_2_1 = data.frame(tissue=tissue_names, total=NA, train=NA, val=NA, test=NA)


for (i in 1:(length(expression_list)-1)) {
  df = expression_list[[i]]
  total = ncol(df)
  train = round(0.7*total)
  val = round(0.2*total)
  test = total - train - val
  
  prop_7_2_1$total[i] = total
  prop_7_2_1$train[i] = train
  prop_7_2_1$val[i] = val
  prop_7_2_1$test[i] = test
  
}
```

```{r echo=FALSE}
prop_7_2_1_long = prop_7_2_1 %>% pivot_longer(c(train, val, test), names_to="set", values_to="value")
```
By the method of trial and error, random sets of proportions were tried and tested on the chosen tissues. The proportions that work the best for modelling the three chosen tissues are: 

1. Training set: 0.7 or 70% 
2. Validation set: 0.2 or 20% 
3. Test set: 0.1 or 10% 

In **Figure 1**, we can see that for each tissue, the test set comfortably exceeds the count of 30 donors. Hence, we can conclude that 0.7-0.2-0.1 makes for most suited and desired proportion for the sets. 


```{r echo=FALSE}
prop_3_tissues = prop_7_2_1_long[c(109,110,111,118,119,120,133,134,135),]
prop_3_tissues$set[prop_3_tissues$set=='train'] = '1train'
prop_3_tissues$set[prop_3_tissues$set=='val'] = '2val'
prop_3_tissues$set[prop_3_tissues$set=='test'] = '3test'
label = rep(c("Train", "Val", "Test"), 3)
```

```{r figs1, echo=FALSE, fig.cap="\\label{fig:figs1}Count of samples per set for chosen tissues"}
ggplot(data=prop_3_tissues, aes(x=set, y=value, fill = tissue)) +
  ggtitle("Sample Sizes for Train, Validation, and Test Set") + 
  xlab("Set") + ylab("Size") +
  theme(axis.text.x = element_text(hjust=1, angle = 30, size=10),
        legend.text = element_text(size=5),
        legend.key.size = unit(0.4, 'cm'),
        legend.position='bottom') +

  geom_bar(stat="identity", show.legend=FALSE) +
  geom_text(aes(label=value), size = 3, vjust=-0.5)+
  facet_wrap(~tissue)
```









## Task 04: Choosing and comparing modelling methods 

The methods we deemed appropriate to initially to explore in this project are Neural Networks and Random Forests. 

### Neural Networks 

Over the past few decades, neural networks modelling has been considered as one of the most powerful tools, and its ability to handle a huge amount of data made it very popular in the literature. Having deep hidden layers in the models has recently become more and more computationally feasible  and such methods has started to surpass classical methods performance in many fields, especially in pattern recognition (Mohammed and Al-Zawi, 2017). 

Deep learning models have become popular in the bioinformatics field. Singh et al. (2016) used a unified CNN framework that automatically learns combinatorial interactions among histone modification marks to predict gene expression. Qi et al. (2012) used a deep multilayer perceptron (MLP) architecture with multitask learning to perform sequence-based protein structure prediction.

By using neural networks, the idea of setting up a lightly parameterised function shaped by human can be forgotten. Instead, it allows us to set up a highly parameterised function that is very flexible and will be conveniently shaped during the learning phase. To put it simply, a deep learning model automatically learns complex functions that map inputs to outputs and rules out the need to use hand-crafted features (Singh et al., 2016). Since the input data of this project has a very large feature dimension, neural network is considered as one of the suitable approaches.

![Neural networks in the human brain](fig 1.png)

The idea of neural network came from our understanding of the brain structure and function. The human brain is made up of billions of basic units called neurons. **Figure 2** illustrates the basic neuron unit. The neuron is made up of dendrites, a cell body and an axon connecting to axon terminals. The brain will receive information or inputs which are then transferred into the cell body through dendrites. The cell body works as the processing unit, where all the learnt information is then transferred into outputs and passed down by the axon. The muscles then receive the outputs from the axon terminals for actions. McCulloch and Pitts first studied this concept in 1943 to form a mathematical model (Bakar and Tahir, 2009).

![Hidden layers of a network](fig 2.png)

**Figure 3** shows a one hidden layer feed forward network with inputs $x_1,â€¦,x_i,$ and output $y_k$. Each input has its own synaptic weight. The weights are then passed onto the hidden layer, which consists of several hidden neurons. A weighted summation of the inputs is performed by each neuron and then it passes a nonlinear activation function.

![An snapshot of GTex data as a neural network](fig 3.png)

In our case, the input will be a matrix of the gene expression of Whole Blood and the output will be a matrix of the gene expression prediction of another tissue. The visualisation of the expected neural network model is shown in **Figure 4**. Note that the number of hidden layers is yet to be decided, and it is for visualisation purpose only. Same thing applies for the number of input and output nodes. The number of input and output nodes in the real model will depend on the number of shared genes between Whole Blood and the other tissue.


### Random Forests (RF)

Over the years, random forests has become a prominently used technique in the field of biology and bioinformatics. Some fields where random forests are used is in classifying different types of samples using gene
expression, identifying diseases associated with particular genes and others. 

![Single decision tree vs. Random Forest](fig 4.png)

A random forest is popularly known to be a classification algorithm, that selects features randomly. It also utilises the concept of bagging samples and majority voting scheme, that make it better than decision trees. Additionally, a random forest is a collection of many decision trees as seen in Figure 6, which is capable of classification and regression tasks. 

Moving to the advantages of using the random forest technique, this machine learning algorithm works best in large and high dimensional data which is very well suited in the context of our project. A large random forest is preferable as it accounts for a robust model, with better accuracy and predictive capabilities. It focuses on selecting the best variable for prediction, and can easily help with identifying variables that aren't very significant to the model. Bootstrapping and ensemble schemes prevent the model from overfitting and hence, pruning trees is not required. Another advantage is that this algorithm accounts for missing values in the data and continues to maintain accuracy. 

In the light of this project, the input will be a matrix of the gene expression of Whole Blood and the expected output is a random forest of the best predictors for gene expression in the other tissue. The model will be built using the training set and tuned for better results over the validation set. 

# Conclusion

On selecting the most appropriate proportions, the training, validation and test sets have an optimal sample size which can now be utilised in the modelling phase. 

Neural networks and random forests are promising methods that can be used when the data is high dimensional and very large. It works out very well with the data being used in this project and has the scope to provide very promising results. 

To conclude this task, our next approach is to use both modelling techniques, report on the findings in both and compare which modelling technique gives has better parameter selection, model accuracy and prediction power. 


